{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "project_01.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "nT0lwc2aLmTu",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "aab977fa-8e8f-402b-dac8-e0c13d05f5ad"
      },
      "source": [
        "%pip install transformers"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting transformers\n",
            "  Downloading transformers-4.12.3-py3-none-any.whl (3.1 MB)\n",
            "\u001b[K     |████████████████████████████████| 3.1 MB 8.3 MB/s \n",
            "\u001b[?25hCollecting sacremoses\n",
            "  Downloading sacremoses-0.0.46-py3-none-any.whl (895 kB)\n",
            "\u001b[K     |████████████████████████████████| 895 kB 34.1 MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.19.5)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2019.12.20)\n",
            "Collecting huggingface-hub<1.0,>=0.1.0\n",
            "  Downloading huggingface_hub-0.1.2-py3-none-any.whl (59 kB)\n",
            "\u001b[K     |████████████████████████████████| 59 kB 4.7 MB/s \n",
            "\u001b[?25hRequirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (21.2)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.62.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers) (4.8.2)\n",
            "Collecting pyyaml>=5.1\n",
            "  Downloading PyYAML-6.0-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (596 kB)\n",
            "\u001b[K     |████████████████████████████████| 596 kB 37.5 MB/s \n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.3.2)\n",
            "Collecting tokenizers<0.11,>=0.10.1\n",
            "  Downloading tokenizers-0.10.3-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (3.3 MB)\n",
            "\u001b[K     |████████████████████████████████| 3.3 MB 29.8 MB/s \n",
            "\u001b[?25hRequirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0,>=0.1.0->transformers) (3.10.0.2)\n",
            "Requirement already satisfied: pyparsing<3,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->transformers) (2.4.7)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers) (3.6.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2021.10.8)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (7.1.2)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.15.0)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.1.0)\n",
            "Installing collected packages: pyyaml, tokenizers, sacremoses, huggingface-hub, transformers\n",
            "  Attempting uninstall: pyyaml\n",
            "    Found existing installation: PyYAML 3.13\n",
            "    Uninstalling PyYAML-3.13:\n",
            "      Successfully uninstalled PyYAML-3.13\n",
            "Successfully installed huggingface-hub-0.1.2 pyyaml-6.0 sacremoses-0.0.46 tokenizers-0.10.3 transformers-4.12.3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6FvFbF72d-R-",
        "outputId": "6f8298b7-b768-41e7-b945-878ecbd2a602"
      },
      "source": [
        "!git clone https://github.com/huggingface/transformers.git"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'transformers'...\n",
            "remote: Enumerating objects: 88991, done.\u001b[K\n",
            "remote: Counting objects: 100% (213/213), done.\u001b[K\n",
            "remote: Compressing objects: 100% (110/110), done.\u001b[K\n",
            "remote: Total 88991 (delta 100), reused 173 (delta 82), pack-reused 88778\u001b[K\n",
            "Receiving objects: 100% (88991/88991), 71.82 MiB | 22.80 MiB/s, done.\n",
            "Resolving deltas: 100% (64092/64092), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i9cOHjukZQt6",
        "outputId": "1b3c895d-907d-4ff7-8f3a-c95f04d2e351"
      },
      "source": [
        "%cd /content"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lrVHQSUcgnhG",
        "outputId": "edaed0d3-d044-46cb-9fa8-fee47c7a0d82"
      },
      "source": [
        "!python gen_token.py"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[00:00:00] Pre-processing files (55 Mo)             ░░░░░░░░                  0%\n",
            "\u001b[2K\u001b[1B\u001b[1A[00:00:00] Pre-processing files (55 Mo)             ░░░░░░░░                  1%\n",
            "\u001b[2K\u001b[1B\u001b[1A[00:00:00] Pre-processing files (55 Mo)             ░░░░░░░░                  2%\n",
            "\u001b[2K\u001b[1B\u001b[1A[00:00:00] Pre-processing files (55 Mo)             ░░░░░░░░                  3%\n",
            "\u001b[2K\u001b[1B\u001b[1A[00:00:01] Pre-processing files (55 Mo)             ░░░░░░░░                  4%\n",
            "\u001b[2K\u001b[1B\u001b[1A[00:00:01] Pre-processing files (55 Mo)             ░░░░░░░░                  5%\n",
            "\u001b[2K\u001b[1B\u001b[1A[00:00:01] Pre-processing files (55 Mo)             ░░░░░░░░                  6%\n",
            "\u001b[2K\u001b[1B\u001b[1A[00:00:01] Pre-processing files (55 Mo)             ░░░░░░░░                  7%\n",
            "\u001b[2K\u001b[1B\u001b[1A[00:00:02] Pre-processing files (55 Mo)             ░░░░░░░░                  8%\n",
            "\u001b[2K\u001b[1B\u001b[1A[00:00:02] Pre-processing files (55 Mo)             ░░░░░░░░                  9%\n",
            "\u001b[2K\u001b[1B\u001b[1A[00:00:02] Pre-processing files (55 Mo)             ░░░░░░░░                 10%\n",
            "\u001b[2K\u001b[1B\u001b[1A[00:00:03] Pre-processing files (55 Mo)             ░░░░░░░░                 11%\n",
            "\u001b[2K\u001b[1B\u001b[1A[00:00:03] Pre-processing files (55 Mo)             ░░░░░░░░                 12%\n",
            "\u001b[2K\u001b[1B\u001b[1A[00:00:03] Pre-processing files (55 Mo)             █░░░░░░░                 13%\n",
            "\u001b[2K\u001b[1B\u001b[1A[00:00:03] Pre-processing files (55 Mo)             █░░░░░░░                 14%\n",
            "\u001b[2K\u001b[1B\u001b[1A[00:00:04] Pre-processing files (55 Mo)             █░░░░░░░                 15%\n",
            "\u001b[2K\u001b[1B\u001b[1A[00:00:04] Pre-processing files (55 Mo)             █░░░░░░░                 16%\n",
            "\u001b[2K\u001b[1B\u001b[1A[00:00:04] Pre-processing files (55 Mo)             █░░░░░░░                 17%\n",
            "\u001b[2K\u001b[1B\u001b[1A[00:00:04] Pre-processing files (55 Mo)             █░░░░░░░                 18%\n",
            "\u001b[2K\u001b[1B\u001b[1A[00:00:05] Pre-processing files (55 Mo)             █░░░░░░░                 19%\n",
            "\u001b[2K\u001b[1B\u001b[1A[00:00:05] Pre-processing files (55 Mo)             █░░░░░░░                 20%\n",
            "\u001b[2K\u001b[1B\u001b[1A[00:00:05] Pre-processing files (55 Mo)             █░░░░░░░                 21%\n",
            "\u001b[2K\u001b[1B\u001b[1A[00:00:06] Pre-processing files (55 Mo)             █░░░░░░░                 22%\n",
            "\u001b[2K\u001b[1B\u001b[1A[00:00:06] Pre-processing files (55 Mo)             █░░░░░░░                 23%\n",
            "\u001b[2K\u001b[1B\u001b[1A[00:00:06] Pre-processing files (55 Mo)             █░░░░░░░                 24%\n",
            "\u001b[2K\u001b[1B\u001b[1A[00:00:06] Pre-processing files (55 Mo)             ██░░░░░░                 25%\n",
            "\u001b[2K\u001b[1B\u001b[1A[00:00:07] Pre-processing files (55 Mo)             ██░░░░░░                 26%\n",
            "\u001b[2K\u001b[1B\u001b[1A[00:00:07] Pre-processing files (55 Mo)             ██░░░░░░                 27%\n",
            "\u001b[2K\u001b[1B\u001b[1A[00:00:07] Pre-processing files (55 Mo)             ██░░░░░░                 28%\n",
            "\u001b[2K\u001b[1B\u001b[1A[00:00:07] Pre-processing files (55 Mo)             ██░░░░░░                 29%\n",
            "\u001b[2K\u001b[1B\u001b[1A[00:00:08] Pre-processing files (55 Mo)             ██░░░░░░                 30%\n",
            "\u001b[2K\u001b[1B\u001b[1A[00:00:08] Pre-processing files (55 Mo)             ██░░░░░░                 31%\n",
            "\u001b[2K\u001b[1B\u001b[1A[00:00:08] Pre-processing files (55 Mo)             ██░░░░░░                 32%\n",
            "\u001b[2K\u001b[1B\u001b[1A[00:00:09] Pre-processing files (55 Mo)             ██░░░░░░                 33%\n",
            "\u001b[2K\u001b[1B\u001b[1A[00:00:09] Pre-processing files (55 Mo)             ██░░░░░░                 34%\n",
            "\u001b[2K\u001b[1B\u001b[1A[00:00:09] Pre-processing files (55 Mo)             ██░░░░░░                 35%\n",
            "\u001b[2K\u001b[1B\u001b[1A[00:00:09] Pre-processing files (55 Mo)             ██░░░░░░                 36%\n",
            "\u001b[2K\u001b[1B\u001b[1A[00:00:10] Pre-processing files (55 Mo)             ██░░░░░░                 37%\n",
            "\u001b[2K\u001b[1B\u001b[1A[00:00:10] Pre-processing files (55 Mo)             ███░░░░░                 38%\n",
            "\u001b[2K\u001b[1B\u001b[1A[00:00:10] Pre-processing files (55 Mo)             ███░░░░░                 39%\n",
            "\u001b[2K\u001b[1B\u001b[1A[00:00:11] Pre-processing files (55 Mo)             ███░░░░░                 40%\n",
            "\u001b[2K\u001b[1B\u001b[1A[00:00:11] Pre-processing files (55 Mo)             ███░░░░░                 41%\n",
            "\u001b[2K\u001b[1B\u001b[1A[00:00:11] Pre-processing files (55 Mo)             ███░░░░░                 42%\n",
            "\u001b[2K\u001b[1B\u001b[1A[00:00:11] Pre-processing files (55 Mo)             ███░░░░░                 43%\n",
            "\u001b[2K\u001b[1B\u001b[1A[00:00:12] Pre-processing files (55 Mo)             ███░░░░░                 44%\n",
            "\u001b[2K\u001b[1B\u001b[1A[00:00:12] Pre-processing files (55 Mo)             ███░░░░░                 45%\n",
            "\u001b[2K\u001b[1B\u001b[1A[00:00:12] Pre-processing files (55 Mo)             ███░░░░░                 46%\n",
            "\u001b[2K\u001b[1B\u001b[1A[00:00:12] Pre-processing files (55 Mo)             ███░░░░░                 47%\n",
            "\u001b[2K\u001b[1B\u001b[1A[00:00:13] Pre-processing files (55 Mo)             ███░░░░░                 48%\n",
            "\u001b[2K\u001b[1B\u001b[1A[00:00:13] Pre-processing files (55 Mo)             ███░░░░░                 49%\n",
            "\u001b[2K\u001b[1B\u001b[1A[00:00:13] Pre-processing files (55 Mo)             ████░░░░                 50%\n",
            "\u001b[2K\u001b[1B\u001b[1A[00:00:14] Pre-processing files (55 Mo)             ████░░░░                 51%\n",
            "\u001b[2K\u001b[1B\u001b[1A[00:00:14] Pre-processing files (55 Mo)             ████░░░░                 52%\n",
            "\u001b[2K\u001b[1B\u001b[1A[00:00:14] Pre-processing files (55 Mo)             ████░░░░                 53%\n",
            "\u001b[2K\u001b[1B\u001b[1A[00:00:14] Pre-processing files (55 Mo)             ████░░░░                 54%\n",
            "\u001b[2K\u001b[1B\u001b[1A[00:00:15] Pre-processing files (55 Mo)             ████░░░░                 55%\n",
            "\u001b[2K\u001b[1B\u001b[1A[00:00:15] Pre-processing files (55 Mo)             ████░░░░                 56%\n",
            "\u001b[2K\u001b[1B\u001b[1A[00:00:15] Pre-processing files (55 Mo)             ████░░░░                 57%\n",
            "\u001b[2K\u001b[1B\u001b[1A[00:00:15] Pre-processing files (55 Mo)             ████░░░░                 58%\n",
            "\u001b[2K\u001b[1B\u001b[1A[00:00:16] Pre-processing files (55 Mo)             ████░░░░                 59%\n",
            "\u001b[2K\u001b[1B\u001b[1A[00:00:16] Pre-processing files (55 Mo)             ████░░░░                 60%\n",
            "\u001b[2K\u001b[1B\u001b[1A[00:00:16] Pre-processing files (55 Mo)             ████░░░░                 61%\n",
            "\u001b[2K\u001b[1B\u001b[1A[00:00:17] Pre-processing files (55 Mo)             ████░░░░                 62%\n",
            "\u001b[2K\u001b[1B\u001b[1A[00:00:17] Pre-processing files (55 Mo)             █████░░░                 63%\n",
            "\u001b[2K\u001b[1B\u001b[1A[00:00:17] Pre-processing files (55 Mo)             █████░░░                 64%\n",
            "\u001b[2K\u001b[1B\u001b[1A[00:00:17] Pre-processing files (55 Mo)             █████░░░                 65%\n",
            "\u001b[2K\u001b[1B\u001b[1A[00:00:18] Pre-processing files (55 Mo)             █████░░░                 66%\n",
            "\u001b[2K\u001b[1B\u001b[1A[00:00:18] Pre-processing files (55 Mo)             █████░░░                 67%\n",
            "\u001b[2K\u001b[1B\u001b[1A[00:00:18] Pre-processing files (55 Mo)             █████░░░                 68%\n",
            "\u001b[2K\u001b[1B\u001b[1A[00:00:18] Pre-processing files (55 Mo)             █████░░░                 69%\n",
            "\u001b[2K\u001b[1B\u001b[1A[00:00:19] Pre-processing files (55 Mo)             █████░░░                 70%\n",
            "\u001b[2K\u001b[1B\u001b[1A[00:00:19] Pre-processing files (55 Mo)             █████░░░                 71%\n",
            "\u001b[2K\u001b[1B\u001b[1A[00:00:19] Pre-processing files (55 Mo)             █████░░░                 72%\n",
            "\u001b[2K\u001b[1B\u001b[1A[00:00:20] Pre-processing files (55 Mo)             █████░░░                 73%\n",
            "\u001b[2K\u001b[1B\u001b[1A[00:00:20] Pre-processing files (55 Mo)             █████░░░                 74%\n",
            "\u001b[2K\u001b[1B\u001b[1A[00:00:20] Pre-processing files (55 Mo)             ██████░░                 75%\n",
            "\u001b[2K\u001b[1B\u001b[1A[00:00:20] Pre-processing files (55 Mo)             ██████░░                 76%\n",
            "\u001b[2K\u001b[1B\u001b[1A[00:00:21] Pre-processing files (55 Mo)             ██████░░                 77%\n",
            "\u001b[2K\u001b[1B\u001b[1A[00:00:21] Pre-processing files (55 Mo)             ██████░░                 78%\n",
            "\u001b[2K\u001b[1B\u001b[1A[00:00:21] Pre-processing files (55 Mo)             ██████░░                 79%\n",
            "\u001b[2K\u001b[1B\u001b[1A[00:00:21] Pre-processing files (55 Mo)             ██████░░                 80%\n",
            "\u001b[2K\u001b[1B\u001b[1A[00:00:22] Pre-processing files (55 Mo)             ██████░░                 81%\n",
            "\u001b[2K\u001b[1B\u001b[1A[00:00:22] Pre-processing files (55 Mo)             ██████░░                 82%\n",
            "\u001b[2K\u001b[1B\u001b[1A[00:00:22] Pre-processing files (55 Mo)             ██████░░                 83%\n",
            "\u001b[2K\u001b[1B\u001b[1A[00:00:23] Pre-processing files (55 Mo)             ██████░░                 84%\n",
            "\u001b[2K\u001b[1B\u001b[1A[00:00:23] Pre-processing files (55 Mo)             ██████░░                 85%\n",
            "\u001b[2K\u001b[1B\u001b[1A[00:00:23] Pre-processing files (55 Mo)             ██████░░                 86%\n",
            "\u001b[2K\u001b[1B\u001b[1A[00:00:23] Pre-processing files (55 Mo)             ██████░░                 87%\n",
            "\u001b[2K\u001b[1B\u001b[1A[00:00:24] Pre-processing files (55 Mo)             ███████░                 88%\n",
            "\u001b[2K\u001b[1B\u001b[1A[00:00:24] Pre-processing files (55 Mo)             ███████░                 89%\n",
            "\u001b[2K\u001b[1B\u001b[1A[00:00:24] Pre-processing files (55 Mo)             ███████░                 90%\n",
            "\u001b[2K\u001b[1B\u001b[1A[00:00:24] Pre-processing files (55 Mo)             ███████░                 91%\n",
            "\u001b[2K\u001b[1B\u001b[1A[00:00:25] Pre-processing files (55 Mo)             ███████░                 92%\n",
            "\u001b[2K\u001b[1B\u001b[1A[00:00:25] Pre-processing files (55 Mo)             ███████░                 93%\n",
            "\u001b[2K\u001b[1B\u001b[1A[00:00:25] Pre-processing files (55 Mo)             ███████░                 94%\n",
            "\u001b[2K\u001b[1B\u001b[1A[00:00:26] Pre-processing files (55 Mo)             ███████░                 95%\n",
            "\u001b[2K\u001b[1B\u001b[1A[00:00:26] Pre-processing files (55 Mo)             ███████░                 96%\n",
            "\u001b[2K\u001b[1B\u001b[1A[00:00:26] Pre-processing files (55 Mo)             ███████░                 97%\n",
            "\u001b[2K\u001b[1B\u001b[1A[00:00:26] Pre-processing files (55 Mo)             ███████░                 98%\n",
            "\u001b[2K\u001b[1B\u001b[1A[00:00:27] Pre-processing files (55 Mo)             ███████░                 99%\n",
            "\u001b[2K\u001b[1B\u001b[1A[00:00:27] Pre-processing files (55 Mo)             ████████                100%\n",
            "[00:00:00] Tokenize words                           ████████ 0        /        0\n",
            "\u001b[2K\u001b[1B\u001b[1A[00:00:00] Tokenize words                           ██░░░░░░ 46761    /   141771\n",
            "\u001b[2K\u001b[1B\u001b[1A[00:00:00] Tokenize words                           █████░░░ 92105    /   141771\n",
            "\u001b[2K\u001b[1B\u001b[1A[00:00:00] Tokenize words                           ███████░ 138866   /   141771\n",
            "\u001b[2K\u001b[1B\u001b[1A[00:00:00] Tokenize words                           ████████ 141771   /   141771\n",
            "\n",
            "\u001b[2K\u001b[1B\u001b[1A[00:00:00] Count pairs                              ░░░░░░░░ 4251     /   141771\n",
            "\u001b[2K\u001b[1B\u001b[1A[00:00:00] Count pairs                              ░░░░░░░░ 7085     /   141771\n",
            "\u001b[2K\u001b[1B\u001b[1A[00:00:00] Count pairs                              ░░░░░░░░ 9919     /   141771\n",
            "\u001b[2K\u001b[1B\u001b[1A[00:00:00] Count pairs                              ░░░░░░░░ 11336    /   141771\n",
            "\u001b[2K\u001b[1B\u001b[1A[00:00:00] Count pairs                              ░░░░░░░░ 12753    /   141771\n",
            "\u001b[2K\u001b[1B\u001b[1A[00:00:00] Count pairs                              ░░░░░░░░ 14170    /   141771\n",
            "\u001b[2K\u001b[1B\u001b[1A[00:00:00] Count pairs                              ░░░░░░░░ 15587    /   141771\n",
            "\u001b[2K\u001b[1B\u001b[1A[00:00:00] Count pairs                              ░░░░░░░░ 17004    /   141771\n",
            "\u001b[2K\u001b[1B\u001b[1A[00:00:00] Count pairs                              █░░░░░░░ 19838    /   141771\n",
            "\u001b[2K\u001b[1B\u001b[1A[00:00:01] Count pairs                              █░░░░░░░ 24089    /   141771\n",
            "\u001b[2K\u001b[1B\u001b[1A[00:00:01] Count pairs                              █░░░░░░░ 26923    /   141771\n",
            "\u001b[2K\u001b[1B\u001b[1A[00:00:01] Count pairs                              █░░░░░░░ 28340    /   141771\n",
            "\u001b[2K\u001b[1B\u001b[1A[00:00:01] Count pairs                              █░░░░░░░ 29757    /   141771\n",
            "\u001b[2K\u001b[1B\u001b[1A[00:00:01] Count pairs                              █░░░░░░░ 31174    /   141771\n",
            "\u001b[2K\u001b[1B\u001b[1A[00:00:01] Count pairs                              █░░░░░░░ 32591    /   141771\n",
            "\u001b[2K\u001b[1B\u001b[1A[00:00:01] Count pairs                              █░░░░░░░ 34008    /   141771\n",
            "\u001b[2K\u001b[1B\u001b[1A[00:00:01] Count pairs                              █░░░░░░░ 35425    /   141771\n",
            "\u001b[2K\u001b[1B\u001b[1A[00:00:01] Count pairs                              ██░░░░░░ 38259    /   141771\n",
            "\u001b[2K\u001b[1B\u001b[1A[00:00:01] Count pairs                              ██░░░░░░ 41093    /   141771\n",
            "\u001b[2K\u001b[1B\u001b[1A[00:00:01] Count pairs                              ██░░░░░░ 43927    /   141771\n",
            "\u001b[2K\u001b[1B\u001b[1A[00:00:02] Count pairs                              ██░░░░░░ 45344    /   141771\n",
            "\u001b[2K\u001b[1B\u001b[1A[00:00:02] Count pairs                              ██░░░░░░ 46761    /   141771\n",
            "\u001b[2K\u001b[1B\u001b[1A[00:00:02] Count pairs                              ██░░░░░░ 48178    /   141771\n",
            "\u001b[2K\u001b[1B\u001b[1A[00:00:02] Count pairs                              ██░░░░░░ 49595    /   141771\n",
            "\u001b[2K\u001b[1B\u001b[1A[00:00:02] Count pairs                              ██░░░░░░ 51012    /   141771\n",
            "\u001b[2K\u001b[1B\u001b[1A[00:00:02] Count pairs                              ██░░░░░░ 52429    /   141771\n",
            "\u001b[2K\u001b[1B\u001b[1A[00:00:02] Count pairs                              ███░░░░░ 55263    /   141771\n",
            "\u001b[2K\u001b[1B\u001b[1A[00:00:02] Count pairs                              ███░░░░░ 58097    /   141771\n",
            "\u001b[2K\u001b[1B\u001b[1A[00:00:02] Count pairs                              ███░░░░░ 60931    /   141771\n",
            "\u001b[2K\u001b[1B\u001b[1A[00:00:02] Count pairs                              ███░░░░░ 63765    /   141771\n",
            "\u001b[2K\u001b[1B\u001b[1A[00:00:03] Count pairs                              ███░░░░░ 65182    /   141771\n",
            "\u001b[2K\u001b[1B\u001b[1A[00:00:03] Count pairs                              ███░░░░░ 66599    /   141771\n",
            "\u001b[2K\u001b[1B\u001b[1A[00:00:03] Count pairs                              ███░░░░░ 68016    /   141771\n",
            "\u001b[2K\u001b[1B\u001b[1A[00:00:03] Count pairs                              ███░░░░░ 69433    /   141771\n",
            "\u001b[2K\u001b[1B\u001b[1A[00:00:03] Count pairs                              ███░░░░░ 70850    /   141771\n",
            "\u001b[2K\u001b[1B\u001b[1A[00:00:03] Count pairs                              ████░░░░ 73684    /   141771\n",
            "\u001b[2K\u001b[1B\u001b[1A[00:00:03] Count pairs                              ████░░░░ 76518    /   141771\n",
            "\u001b[2K\u001b[1B\u001b[1A[00:00:03] Count pairs                              ████░░░░ 79352    /   141771\n",
            "\u001b[2K\u001b[1B\u001b[1A[00:00:03] Count pairs                              ████░░░░ 80769    /   141771\n",
            "\u001b[2K\u001b[1B\u001b[1A[00:00:03] Count pairs                              ████░░░░ 82186    /   141771\n",
            "\u001b[2K\u001b[1B\u001b[1A[00:00:03] Count pairs                              ████░░░░ 83603    /   141771\n",
            "\u001b[2K\u001b[1B\u001b[1A[00:00:04] Count pairs                              ████░░░░ 85020    /   141771\n",
            "\u001b[2K\u001b[1B\u001b[1A[00:00:04] Count pairs                              ████░░░░ 86437    /   141771\n",
            "\u001b[2K\u001b[1B\u001b[1A[00:00:04] Count pairs                              ████░░░░ 87854    /   141771\n",
            "\u001b[2K\u001b[1B\u001b[1A[00:00:04] Count pairs                              █████░░░ 90688    /   141771\n",
            "\u001b[2K\u001b[1B\u001b[1A[00:00:04] Count pairs                              █████░░░ 92105    /   141771\n",
            "\u001b[2K\u001b[1B\u001b[1A[00:00:04] Count pairs                              █████░░░ 94939    /   141771\n",
            "\u001b[2K\u001b[1B\u001b[1A[00:00:04] Count pairs                              █████░░░ 97773    /   141771\n",
            "\u001b[2K\u001b[1B\u001b[1A[00:00:04] Count pairs                              █████░░░ 99190    /   141771\n",
            "\u001b[2K\u001b[1B\u001b[1A[00:00:04] Count pairs                              █████░░░ 100607   /   141771\n",
            "\u001b[2K\u001b[1B\u001b[1A[00:00:04] Count pairs                              █████░░░ 102024   /   141771\n",
            "\u001b[2K\u001b[1B\u001b[1A[00:00:05] Count pairs                              █████░░░ 103441   /   141771\n",
            "\u001b[2K\u001b[1B\u001b[1A[00:00:05] Count pairs                              █████░░░ 104858   /   141771\n",
            "\u001b[2K\u001b[1B\u001b[1A[00:00:05] Count pairs                              ██████░░ 107692   /   141771\n",
            "\u001b[2K\u001b[1B\u001b[1A[00:00:05] Count pairs                              ██████░░ 109109   /   141771\n",
            "\u001b[2K\u001b[1B\u001b[1A[00:00:05] Count pairs                              ██████░░ 110526   /   141771\n",
            "\u001b[2K\u001b[1B\u001b[1A[00:00:05] Count pairs                              ██████░░ 113360   /   141771\n",
            "\u001b[2K\u001b[1B\u001b[1A[00:00:05] Count pairs                              ██████░░ 116194   /   141771\n",
            "\u001b[2K\u001b[1B\u001b[1A[00:00:05] Count pairs                              ██████░░ 117611   /   141771\n",
            "\u001b[2K\u001b[1B\u001b[1A[00:00:05] Count pairs                              ██████░░ 119028   /   141771\n",
            "\u001b[2K\u001b[1B\u001b[1A[00:00:05] Count pairs                              ██████░░ 120445   /   141771\n",
            "\u001b[2K\u001b[1B\u001b[1A[00:00:05] Count pairs                              ██████░░ 121862   /   141771\n",
            "\u001b[2K\u001b[1B\u001b[1A[00:00:06] Count pairs                              ██████░░ 123279   /   141771\n",
            "\u001b[2K\u001b[1B\u001b[1A[00:00:06] Count pairs                              ███████░ 126113   /   141771\n",
            "\u001b[2K\u001b[1B\u001b[1A[00:00:06] Count pairs                              ███████░ 127530   /   141771\n",
            "\u001b[2K\u001b[1B\u001b[1A[00:00:06] Count pairs                              ███████░ 130364   /   141771\n",
            "\u001b[2K\u001b[1B\u001b[1A[00:00:06] Count pairs                              ███████░ 133198   /   141771\n",
            "\u001b[2K\u001b[1B\u001b[1A[00:00:06] Count pairs                              ███████░ 134615   /   141771\n",
            "\u001b[2K\u001b[1B\u001b[1A[00:00:06] Count pairs                              ███████░ 136032   /   141771\n",
            "\u001b[2K\u001b[1B\u001b[1A[00:00:06] Count pairs                              ███████░ 137449   /   141771\n",
            "\u001b[2K\u001b[1B\u001b[1A[00:00:06] Count pairs                              ███████░ 138866   /   141771\n",
            "\u001b[2K\u001b[1B\u001b[1A[00:00:06] Count pairs                              ███████░ 140283   /   141771\n",
            "\u001b[2K\u001b[1B\u001b[1A[00:00:07] Count pairs                              ███████░ 141700   /   141771\n",
            "\u001b[2K\u001b[1B\u001b[1A[00:00:07] Count pairs                              ████████ 141771   /   141771\n",
            "\n",
            "\u001b[2K\u001b[1B\u001b[1A[00:00:00] Compute merges                           ░░░░░░░░ 520      /    52000\n",
            "\u001b[2K\u001b[1B\u001b[1A[00:00:00] Compute merges                           ░░░░░░░░ 1040     /    52000\n",
            "\u001b[2K\u001b[1B\u001b[1A[00:00:01] Compute merges                           ░░░░░░░░ 1560     /    52000\n",
            "\u001b[2K\u001b[1B\u001b[1A[00:00:01] Compute merges                           ░░░░░░░░ 2600     /    52000\n",
            "\u001b[2K\u001b[1B\u001b[1A[00:00:01] Compute merges                           ░░░░░░░░ 3640     /    52000\n",
            "\u001b[2K\u001b[1B\u001b[1A[00:00:01] Compute merges                           ░░░░░░░░ 5200     /    52000\n",
            "\u001b[2K\u001b[1B\u001b[1A[00:00:01] Compute merges                           █░░░░░░░ 7280     /    52000\n",
            "\u001b[2K\u001b[1B\u001b[1A[00:00:01] Compute merges                           █░░░░░░░ 9360     /    52000\n",
            "\u001b[2K\u001b[1B\u001b[1A[00:00:01] Compute merges                           █░░░░░░░ 11960    /    52000\n",
            "\u001b[2K\u001b[1B\u001b[1A[00:00:01] Compute merges                           ██░░░░░░ 14560    /    52000\n",
            "\u001b[2K\u001b[1B\u001b[1A[00:00:01] Compute merges                           ██░░░░░░ 17160    /    52000\n",
            "\u001b[2K\u001b[1B\u001b[1A[00:00:01] Compute merges                           ███░░░░░ 20280    /    52000\n",
            "\u001b[2K\u001b[1B\u001b[1A[00:00:01] Compute merges                           ███░░░░░ 24440    /    52000\n",
            "\u001b[2K\u001b[1B\u001b[1A[00:00:01] Compute merges                           ████░░░░ 28080    /    52000\n",
            "\u001b[2K\u001b[1B\u001b[1A[00:00:01] Compute merges                           ████░░░░ 32240    /    52000\n",
            "\u001b[2K\u001b[1B\u001b[1A[00:00:02] Compute merges                           █████░░░ 36920    /    52000\n",
            "\u001b[2K\u001b[1B\u001b[1A[00:00:02] Compute merges                           ██████░░ 42120    /    52000\n",
            "\u001b[2K\u001b[1B\u001b[1A[00:00:02] Compute merges                           ██████░░ 45240    /    52000\n",
            "\u001b[2K\u001b[1B\u001b[1A[00:00:02] Compute merges                           ███████░ 50440    /    52000\n",
            "\u001b[2K\u001b[1B\u001b[1A[00:00:02] Compute merges                           ████████ 51659    /    51659\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ImWAIZrWiJli",
        "outputId": "315c0906-802f-4bff-c7ae-353cc9c63158"
      },
      "source": [
        "%cd transformers/examples/pytorch/language-modeling/"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/transformers/examples/pytorch/language-modeling\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OKXeDnvnLGKK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4998db95-a268-4bb8-a896-74ae98844757"
      },
      "source": [
        "from tokenizers.implementations import ByteLevelBPETokenizer\n",
        "from tokenizers.processors import ByteLevel\n",
        "\n",
        "tokenizer = ByteLevelBPETokenizer(\n",
        "  \"poem/vocab.json\",\n",
        "  \"poem/merges.txt\"\n",
        ")\n",
        "tokenizer._tokenizer.post_processor = ByteLevel(\n",
        "    trim_offset = True\n",
        ")\n",
        "tokenizer.enable_truncation(max_length=512)\n",
        "\n",
        "print(\n",
        "    tokenizer.encode(\"Hello world.\")\n",
        ")\n"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Encoding(num_tokens=5, attributes=[ids, type_ids, tokens, offsets, attention_mask, special_tokens_mask, overflowing])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Euv28XdFMh5W"
      },
      "source": [
        "from transformers import GPT2Config\n",
        "config = GPT2Config()"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "riqaBd9nNRYE",
        "outputId": "c419a34e-5bcc-4721-cbe6-192dd49b7bec"
      },
      "source": [
        "from transformers import GPT2Tokenizer\n",
        "tokenizer = GPT2Tokenizer.from_pretrained(\"poem\", max_len=512)\n",
        "tokenizer.add_special_tokens({'bos_token': '<BOS>', 'eos_token': '<EOS>','pad_token': '[PAD]'})"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "3"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CY4teA_OOXOo"
      },
      "source": [
        "from transformers import GPT2LMHeadModel\n",
        "model = GPT2LMHeadModel(config=config)"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jlfYXaGdRqSz",
        "outputId": "1900b669-2d67-4a9f-c00f-86eee9fa4391"
      },
      "source": [
        "from transformers import LineByLineTextDataset\n",
        "dataset = LineByLineTextDataset(\n",
        "    tokenizer=tokenizer,\n",
        "    file_path=\"./limericks_with_syllables_new.txt\",\n",
        "    block_size=128,\n",
        ")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/transformers/data/datasets/language_modeling.py:125: FutureWarning: This dataset will be removed from the library soon, preprocessing should be handled with the 🤗 Datasets library. You can have a look at this example script for pointers: https://github.com/huggingface/transformers/blob/master/examples/pytorch/language-modeling/run_mlm.py\n",
            "  FutureWarning,\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zG4yurwvSxww"
      },
      "source": [
        "from transformers import DataCollatorForLanguageModeling\n",
        "\n",
        "data_collator = DataCollatorForLanguageModeling(\n",
        "    tokenizer=tokenizer, mlm=False\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 463
        },
        "id": "Qtkb6C3hTyyc",
        "outputId": "a0703261-0bfd-4b36-a176-66bff71bd922"
      },
      "source": [
        "from transformers import Trainer, TrainingArguments\n",
        "\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"./output\",\n",
        "    overwrite_output_dir=True,\n",
        "    num_train_epochs=1,\n",
        "    per_gpu_train_batch_size=64,\n",
        "    save_steps=10_000,\n",
        "    save_total_limit=2,\n",
        "    prediction_loss_only=True,\n",
        ")\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    data_collator=data_collator,\n",
        "    train_dataset=dataset,\n",
        ")"
      ],
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "PyTorch: setting up devices\n",
            "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-55-11de6891238b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[0margs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtraining_args\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[0mdata_collator\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdata_collator\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m     \u001b[0mtrain_dataset\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m )\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/transformers/trainer.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, model, args, data_collator, train_dataset, eval_dataset, tokenizer, model_init, compute_metrics, callbacks, optimizers)\u001b[0m\n\u001b[1;32m    289\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    290\u001b[0m         \u001b[0;31m# Seed must be set before instantiating the model when using model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 291\u001b[0;31m         \u001b[0mset_seed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mseed\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    292\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhp_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    293\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdeepspeed\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/transformers/trainer_utils.py\u001b[0m in \u001b[0;36mset_seed\u001b[0;34m(seed)\u001b[0m\n\u001b[1;32m     59\u001b[0m     \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mseed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mseed\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mis_torch_available\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 61\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmanual_seed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mseed\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     62\u001b[0m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmanual_seed_all\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mseed\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m         \u001b[0;31m# ^^ safe to call this function even if cuda is not available\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/random.py\u001b[0m in \u001b[0;36mmanual_seed\u001b[0;34m(seed)\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_is_in_bad_fork\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 40\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmanual_seed_all\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mseed\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     41\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mdefault_generator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmanual_seed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mseed\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/cuda/random.py\u001b[0m in \u001b[0;36mmanual_seed_all\u001b[0;34m(seed)\u001b[0m\n\u001b[1;32m    111\u001b[0m             \u001b[0mdefault_generator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmanual_seed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mseed\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 113\u001b[0;31m     \u001b[0m_lazy_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mseed_all\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    114\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    115\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/cuda/__init__.py\u001b[0m in \u001b[0;36m_lazy_call\u001b[0;34m(callable, **kwargs)\u001b[0m\n\u001b[1;32m    151\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_lazy_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcallable\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mis_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 153\u001b[0;31m         \u001b[0mcallable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    154\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    155\u001b[0m         \u001b[0;31m# TODO(torch_deploy): this accesses linecache, which attempts to read the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/cuda/random.py\u001b[0m in \u001b[0;36mcb\u001b[0;34m()\u001b[0m\n\u001b[1;32m    109\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    110\u001b[0m             \u001b[0mdefault_generator\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdefault_generators\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 111\u001b[0;31m             \u001b[0mdefault_generator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmanual_seed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mseed\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    112\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    113\u001b[0m     \u001b[0m_lazy_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mseed_all\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: CUDA error: device-side assert triggered\nCUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1."
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 541
        },
        "id": "WSop9j3KUVtL",
        "outputId": "856cc165-4258-4fbf-e0fc-b9ec95db34e3"
      },
      "source": [
        "trainer.train()"
      ],
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Using deprecated `--per_gpu_train_batch_size` argument which will be removed in a future version. Using `--per_device_train_batch_size` is preferred.\n",
            "Using deprecated `--per_gpu_train_batch_size` argument which will be removed in a future version. Using `--per_device_train_batch_size` is preferred.\n",
            "***** Running training *****\n",
            "  Num examples = 530428\n",
            "  Num Epochs = 1\n",
            "  Instantaneous batch size per device = 8\n",
            "  Total train batch size (w. parallel, distributed & accumulation) = 64\n",
            "  Gradient Accumulation steps = 1\n",
            "  Total optimization steps = 8288\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-56-3435b262f1ae>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/transformers/trainer.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   1252\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1253\u001b[0m         \u001b[0;31m# tr_loss is a tensor to avoid synchronization of TPUs through .item()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1254\u001b[0;31m         \u001b[0mtr_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0.0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1255\u001b[0m         \u001b[0;31m# _total_loss_scalar is updated everytime .item() has to be called on tr_loss and stores the sum of all losses\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1256\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_total_loss_scalar\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: CUDA error: device-side assert triggered\nCUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1."
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0OyuMXuoUm72"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}