{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "pr.ipynb",
      "provenance": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zYg_nxcuPYX1",
        "outputId": "0d543fd4-20a0-40b2-901a-9264cc1bc473"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/gdrive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZO-BbQrafTpr"
      },
      "source": [
        "# Libraries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b5k5hu11P6tt",
        "outputId": "20511ede-bb31-412f-80fc-da294f52073e"
      },
      "source": [
        "!pip install transformers\n",
        "!pip install syllabipy"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting transformers\n",
            "  Downloading transformers-4.12.5-py3-none-any.whl (3.1 MB)\n",
            "\u001b[K     |████████████████████████████████| 3.1 MB 4.3 MB/s \n",
            "\u001b[?25hRequirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers) (4.8.2)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.62.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.4.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (21.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2019.12.20)\n",
            "Collecting huggingface-hub<1.0,>=0.1.0\n",
            "  Downloading huggingface_hub-0.1.2-py3-none-any.whl (59 kB)\n",
            "\u001b[K     |████████████████████████████████| 59 kB 7.7 MB/s \n",
            "\u001b[?25hCollecting pyyaml>=5.1\n",
            "  Downloading PyYAML-6.0-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (596 kB)\n",
            "\u001b[K     |████████████████████████████████| 596 kB 95.2 MB/s \n",
            "\u001b[?25hCollecting sacremoses\n",
            "  Downloading sacremoses-0.0.46-py3-none-any.whl (895 kB)\n",
            "\u001b[K     |████████████████████████████████| 895 kB 58.7 MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.19.5)\n",
            "Collecting tokenizers<0.11,>=0.10.1\n",
            "  Downloading tokenizers-0.10.3-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (3.3 MB)\n",
            "\u001b[K     |████████████████████████████████| 3.3 MB 55.8 MB/s \n",
            "\u001b[?25hRequirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0,>=0.1.0->transformers) (3.10.0.2)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->transformers) (3.0.6)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers) (3.6.0)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2021.10.8)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (7.1.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.1.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.15.0)\n",
            "Installing collected packages: pyyaml, tokenizers, sacremoses, huggingface-hub, transformers\n",
            "  Attempting uninstall: pyyaml\n",
            "    Found existing installation: PyYAML 3.13\n",
            "    Uninstalling PyYAML-3.13:\n",
            "      Successfully uninstalled PyYAML-3.13\n",
            "Successfully installed huggingface-hub-0.1.2 pyyaml-6.0 sacremoses-0.0.46 tokenizers-0.10.3 transformers-4.12.5\n",
            "Collecting syllabipy\n",
            "  Downloading syllabipy-0.2.tar.gz (3.9 kB)\n",
            "Building wheels for collected packages: syllabipy\n",
            "  Building wheel for syllabipy (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for syllabipy: filename=syllabipy-0.2-py3-none-any.whl size=5813 sha256=492d4756d605641acdda4afd27e325967ec859b6c561e72e2a30d6c055fa5fb6\n",
            "  Stored in directory: /root/.cache/pip/wheels/5a/43/1a/9078e0df36fa76df8c584c20b0eeb924ad8686d240b1a9646a\n",
            "Successfully built syllabipy\n",
            "Installing collected packages: syllabipy\n",
            "Successfully installed syllabipy-0.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ff9N3QJeQjvo"
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd \n",
        "\n",
        "import random\n",
        "import time\n",
        "import datetime\n",
        "\n",
        "import torch\n",
        "from torch.utils.data import Dataset, random_split, DataLoader, RandomSampler, SequentialSampler\n",
        "\n",
        "from tqdm import tqdm\n",
        "\n",
        "from syllabipy.sonoripy import SonoriPy\n",
        "\n",
        "from tokenizers import ByteLevelBPETokenizer\n",
        "from tokenizers import BertWordPieceTokenizer\n",
        "\n",
        "from transformers import GPT2Tokenizer, GPT2LMHeadModel, GPT2Config, AdamW, get_linear_schedule_with_warmup\n",
        "from transformers import BertTokenizer"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6p5MZWW5fU66"
      },
      "source": [
        "# Train Tokenizer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Lpq1i5n7QoAW",
        "outputId": "59ee3dfa-a83f-4e5e-be55-f01ef1630009"
      },
      "source": [
        "# Train tokenizer\n",
        "\n",
        "paths = [\"/content/gdrive/MyDrive/pr/limericks_end_with_[SEP]_sep_with_-_and_$.txt\"]\n",
        "\n",
        "# Initialize a tokenizer\n",
        "tokenizer = BertWordPieceTokenizer()\n",
        "\n",
        "special_tokens=['[SEP]', '-', '$']\n",
        "print(special_tokens)\n",
        "\n",
        "# Customize training\n",
        "tokenizer.train(files=paths, vocab_size=30_000, min_frequency=2, special_tokens=special_tokens)\n",
        "\n",
        "# special_tokens_dict = {'bos_token': '<BOS>', 'eos_token': '<EOS>', 'pad_token': '<PAD>'}\n",
        "# tokenizer.add_special_tokens(special_tokens_dict)\n",
        "\n",
        "# Save files to disk\n",
        "tokenizer.save_model(\"/content/gdrive/MyDrive/pr/tokenizers\", \"tokenizerSyllables\")"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['[SEP]', '-', '$']\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['/content/gdrive/MyDrive/pr/tokenizers/tokenizerSyllables-vocab.txt']"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oN7mPB20fWkS"
      },
      "source": [
        "# Load Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "56IpRDTHclNm",
        "outputId": "610b673a-7032-4d09-aded-d4e254a8b108"
      },
      "source": [
        "poem_df = pd.read_csv(\"/content/gdrive/MyDrive/pr/limericks_end_with_[SEP]_sep_with_-_and_$.txt\")\n",
        "poem_df = poem_df.fillna(\"\")\n",
        "print(poem_df)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "      capn $ jack $ was $ was hed $ o ver $ the $ si de - his $ crew $ searc hed $ but $ found $ not $ hair $ nor $ hi de - no $ lon ger $ the $ helm - but $ the $ deep $ bent hic $ realm - is $ whe re $ jack $ will $ fo re ver $ re si de [SEP]- \n",
            "0      as $ a $ soup $ bis que $ is $ best $ when $ s...                                                                                                                                                                                              \n",
            "1      sim ply $ add $ to $ the $ grasp $ of $ a $ rh...                                                                                                                                                                                              \n",
            "2      a beds $ whe re $ yo u $ sleep $ in $ the $ ni...                                                                                                                                                                                              \n",
            "3      a $ smi ling $ yo ung $ fel low $ from $ spain...                                                                                                                                                                                              \n",
            "4      the $ man $ who $ be co mes $ al co ho lic - i...                                                                                                                                                                                              \n",
            "...                                                  ...                                                                                                                                                                                              \n",
            "88506  the $ sto rys $ in $ front $ of $ our $ no ses...                                                                                                                                                                                              \n",
            "88507  un ders tan ding $ the $ bi ble $ is $ hard - ...                                                                                                                                                                                              \n",
            "88508  di ver ti cu la $ ma king $ yo u $ sick $ yo u...                                                                                                                                                                                              \n",
            "88509  un $ bal lo $ in $ masc he ra $ what - is $ th...                                                                                                                                                                                              \n",
            "88510  i $ said $ joe $ dad dy $ thinks $ that $ yo u...                                                                                                                                                                                              \n",
            "\n",
            "[88511 rows x 1 columns]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hMruqHnUfYEi"
      },
      "source": [
        "# Hyperparameters"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZVCFQyVvQlR1"
      },
      "source": [
        "random_seed = 73\n",
        "batch_size = 32\n",
        "epochs = 8\n",
        "max_len = 200\n",
        "\n",
        "learning_rate = 1e-4\n",
        "eps = 1e-8\n",
        "warmup_steps = 50\n",
        "\n",
        "device = torch.device('cuda')"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VJU1_6pzR1OD",
        "outputId": "72703c9e-9aaa-4117-df7d-2c6cfaf7921f"
      },
      "source": [
        "torch.cuda.manual_seed_all(random_seed)\n",
        "random.seed(random_seed)\n",
        "np.random.seed(random_seed)\n",
        "torch.manual_seed(random_seed)"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<torch._C.Generator at 0x7ff912e91e90>"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y-MuzpjBRyf9",
        "outputId": "431ff5c0-cc93-4feb-f295-0c3e9e6ddaed"
      },
      "source": [
        "tokenizer = BertTokenizer.from_pretrained(\"/content/gdrive/MyDrive/pr/tokenizers/tokenizerSyllables-vocab.txt\")\n",
        "print(\"len(tokenizer) = \", len(tokenizer))"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:1645: FutureWarning: Calling BertTokenizer.from_pretrained() with the path to a single file or url is deprecated and won't be possible anymore in v5. Use a model identifier or the path to a directory instead.\n",
            "  FutureWarning,\n",
            "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "len(tokenizer) =  15112\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "doQBKkIsifDa"
      },
      "source": [
        "# Dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EIn0Jtmde4Mu"
      },
      "source": [
        "class PoemDataset(Dataset):\n",
        "    \n",
        "    def __init__(self, data, tokenizer, max_length=max_len):\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_length = max_length\n",
        "        self.data = data\n",
        "        \n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "    \n",
        "    def __getitem__(self, idx):\n",
        "        encodings_dict = self.tokenizer(self.data[idx],\n",
        "                                        truncation=True,\n",
        "                                        max_length=self.max_length,\n",
        "                                        padding='max_length'\n",
        "                                        )\n",
        "        input_ids = torch.tensor(encodings_dict['input_ids'])\n",
        "        attention_mask = torch.tensor(encodings_dict['attention_mask'])\n",
        "        return input_ids, attention_mask\n",
        "\n",
        "poem_dataset = PoemDataset(poem_df.iloc[:, 0].values, tokenizer, max_len)"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6r5x0xetikay"
      },
      "source": [
        "# Dataloader"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yOFnKAZUS--M"
      },
      "source": [
        "poem_dataloader = DataLoader(poem_dataset, batch_size=batch_size)"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "emU8gE2rimav"
      },
      "source": [
        "# Model Definition"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ut90DCyOTC_z"
      },
      "source": [
        "configuration = GPT2Config(vocab_size=len(tokenizer), n_positions=max_len)\n",
        "poem_model = GPT2LMHeadModel(config=configuration)\n",
        "poem_model.resize_token_embeddings(len(tokenizer))\n",
        "\n",
        "optimizer = AdamW(poem_model.parameters(), lr=learning_rate, eps=eps)\n",
        "\n",
        "total_steps = len(poem_dataloader) * epochs\n",
        "scheduler = get_linear_schedule_with_warmup(optimizer,\n",
        "                                            num_warmup_steps=warmup_steps,\n",
        "                                            num_training_steps=total_steps)\n",
        "\n",
        "poem_model = poem_model.to(device)"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Gs4RdbrCiF9w"
      },
      "source": [
        "def generate_poems(poem_model):\n",
        "    prompt = \"[CLS]\"\n",
        "    generated = torch.tensor(tokenizer.encode(prompt)).unsqueeze(0)\n",
        "    generated = generated.to(device)\n",
        "\n",
        "    poem_model.eval()\n",
        "    sample_outputs = poem_model.generate(\n",
        "                                    generated, \n",
        "                                    do_sample=True,   \n",
        "                                    top_k=50, \n",
        "                                    max_length=max_len,\n",
        "                                    top_p=0.95, \n",
        "                                    num_return_sequences=3\n",
        "                                    )\n",
        "\n",
        "    for i, sample_output in enumerate(sample_outputs):\n",
        "        sample_output = tokenizer.decode(sample_output, skip_special_tokens=True)\n",
        "        sample_output = sample_output.replace(\" \", \"\").replace(\"$\", \" \").replace(\"-\", \" - \")\n",
        "        print(\"{}: {}\\n\\n\".format(i, sample_output))"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T89LgwtpTL-g",
        "outputId": "ae37704d-59c7-4f68-bf23-dae3b5513a2d"
      },
      "source": [
        "model_weights_file = '/content/gdrive/MyDrive/pr/model_weights_20211127_1_epoch7.pth'\n",
        "poem_model.load_state_dict(torch.load(model_weights_file))\n",
        "\n",
        "for epoch in range(8, 15):\n",
        "\n",
        "    print(f'Epoch {epoch + 1} of {epochs}')\n",
        "\n",
        "    total_train_loss = 0\n",
        "    poem_model.train()\n",
        "\n",
        "    with tqdm(poem_dataloader) as t:\n",
        "      for step, batch in enumerate(t):\n",
        "\n",
        "        b_input_ids = batch[0].to(device)\n",
        "        b_labels = batch[0].to(device)\n",
        "        b_masks = batch[1].to(device)\n",
        "\n",
        "        poem_model.zero_grad()        \n",
        "\n",
        "        outputs = poem_model(b_input_ids,\n",
        "                            labels=b_labels,\n",
        "                            token_type_ids=None,\n",
        "                            attention_mask=b_masks)\n",
        "\n",
        "        loss = outputs[0]  \n",
        "\n",
        "        batch_loss = loss.item()\n",
        "        total_train_loss += batch_loss\n",
        "\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        scheduler.step()\n",
        "\n",
        "    avg_train_loss = total_train_loss / len(poem_dataloader)       \n",
        "\n",
        "    print(f'Average Training Loss: {avg_train_loss}.')\n",
        "\n",
        "    torch.save(poem_model.state_dict(), '/content/gdrive/MyDrive/pr/model_weights_20211127_1_epoch%d.pth' % epoch)\n",
        "\n",
        "    generate_poems(poem_model)"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 9 of 8\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 2766/2766 [35:56<00:00,  1.28it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average Training Loss: 0.8333827336025859.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0:  in the breeze - was the corol i hate - but i never could wait - the colonos he said was degrees - \n",
            "\n",
            "\n",
            "1:  - made me shine like a fine disarray - not a rope but a lot - the names mine for a lot - an a favorite every fine day\n",
            "\n",
            "\n",
            "2:  economys african tree - and the colors these likely should be - youre delicious with ease - and as hard as it please - that this genus is not very free\n",
            "\n",
            "\n",
            "Epoch 10 of 8\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 2766/2766 [35:55<00:00,  1.28it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average Training Loss: 0.7890202964421036.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0:  as baseball can be - useful baseballs use column youll see - but a navy among - its a double hue hung - to a heaviest avenue  - \n",
            "\n",
            "\n",
            "1:  with decolate names - from the glaciers all over his aims - to make music like this - though a copycat miss - and the absence of tales like the aims\n",
            "\n",
            "\n",
            "2:  in darkness ive seen - and the highest of leaves on the screen - ive been growing my style - to relieve in the nile - it seems strange that ive always been seen\n",
            "\n",
            "\n",
            "Epoch 11 of 8\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 2766/2766 [35:55<00:00,  1.28it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average Training Loss: 0.736409771920802.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0:  sunday weve built a new dale - where the world isnt easily stale - we are forced to the skies - but the timehonored guys - say our honeymoon salesmen are stale\n",
            "\n",
            "\n",
            "1:  - i keep there on his paneline my day - there is no need to laugh - only fools all like half - and he finds as he passes one way - \n",
            "\n",
            "\n",
            "2:  - and the languish they think is the way - wheres the ague or syrup - they think that theyre teeming  - onions a consequence of prey - \n",
            "\n",
            "\n",
            "Epoch 12 of 8\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 2766/2766 [35:55<00:00,  1.28it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average Training Loss: 0.6846185267316373.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0:    a biteaus - i think ill make a - change to get things youll see - a good buyer though widely desire - \n",
            "\n",
            "\n",
            "1:  haline fish in the sea  - are they sharp overstation - wheres high devoration  - oh where every item will be - \n",
            "\n",
            "\n",
            "2:  you must see - is the fruit that fits straight from the tree - from a species of mine - its the amazon sign - a mirage agaric that be - \n",
            "\n",
            "\n",
            "Epoch 13 of 8\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 2766/2766 [35:55<00:00,  1.28it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average Training Loss: 0.6367506664125108.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0:  was well like a mile  - not yet linen again - he was hopin and then - hed opine you to make a sweet smile lexicographers - \n",
            "\n",
            "\n",
            "1:  will get wise - you think it excites us - when you add when youre knight - an antistress it helps to divide us - \n",
            "\n",
            "\n",
            "2:  on the fish fish - if i have it with ease - ill just have a disease - wheres my basic a coupler of these - \n",
            "\n",
            "\n",
            "Epoch 14 of 8\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 2766/2766 [35:56<00:00,  1.28it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average Training Loss: 0.5955480743450094.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0:  to thinki goodbye - with no onion or ring - you can see whats athing - if you please see a good one dont fly - \n",
            "\n",
            "\n",
            "1:  distractions taboose - like burgeoning plants - paramesation astroys - and a term that enhances the treatments i use - \n",
            "\n",
            "\n",
            "2:  in the green fishing trees - will grow on the roots - if they dont give you these - if you learn that the best guarantees - \n",
            "\n",
            "\n",
            "Epoch 15 of 8\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 2766/2766 [35:54<00:00,  1.28it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average Training Loss: 0.5651384732340525.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0:  - called fishermen called on the isle - though the wind in its tribe - might bring disrespected libe  - headless airlines a very good style - \n",
            "\n",
            "\n",
            "1:  - is a concert in the uk - he was given to stop  - but at least for the cop - its deception  ill bet this ill pay - \n",
            "\n",
            "\n",
            "2:  - would rap in the mexican skies - on the beach as theyre pressed - to the sand on their breast - given moisture with maximal prize - \n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZTuEPLNqQ9hz"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}